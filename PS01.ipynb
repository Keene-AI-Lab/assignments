{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10114315,
          "sourceType": "datasetVersion",
          "datasetId": 6240249
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opensmile --quiet"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T19:52:13.444068Z",
          "iopub.execute_input": "2025-03-05T19:52:13.444629Z",
          "iopub.status.idle": "2025-03-05T19:52:22.074137Z",
          "shell.execute_reply.started": "2025-03-05T19:52:13.444583Z",
          "shell.execute_reply": "2025-03-05T19:52:22.072762Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5izMx9Ea6p9H",
        "outputId": "66b1c270-1221-4740-d958-be1581288a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/996.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.6/996.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m993.3/996.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.0/996.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import pickle\n",
        "import zipfile\n",
        "import librosa\n",
        "import opensmile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T07:11:46.167017Z",
          "iopub.execute_input": "2025-03-02T07:11:46.167335Z",
          "iopub.status.idle": "2025-03-02T07:11:46.172988Z",
          "shell.execute_reply.started": "2025-03-02T07:11:46.167313Z",
          "shell.execute_reply": "2025-03-02T07:11:46.172098Z"
        },
        "id": "kf-GBTNz6p9M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "U7uQj2iO6p9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Google Colab or Local Notebook\n",
        "\n",
        "For those who prefer working on their local machine or Google Colab, the cell below downloads the dataset from Google Drive, extracts it to your current working directory, and defines the `dataset_dir` variable pointing to the extracted folder. This provides a simple, reproducible way to load the dataset without needing to work directly from a Kaggle notebook.\n",
        "<br>\n",
        "\n",
        "**Note:** The dataset is aboout 16 MB.\n"
      ],
      "metadata": {
        "id": "hW8crrVB6p9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install gdown if not already installed\n",
        "!pip install -q gdown\n",
        "\n",
        "# download the fsdd.zip file from Google Drive using its file ID\n",
        "!gdown 1pt75Fs4APfvxQ1FiuubEWkRJp7tjwndW\n",
        "\n",
        "# unzip the downloaded file (assumes the zip file is named \"fsdd.zip\")\n",
        "zip_filename = \"fsdd.zip\"\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n",
        "\n",
        "# remove the zip file after extraction\n",
        "os.remove(zip_filename)\n",
        "\n",
        "# define the dataset directory variable (assumes the extracted folder is named \"fsdd\")\n",
        "dataset_dir = os.path.join(os.getcwd(), \"fsdd\")\n",
        "print(\"Dataset directory:\", dataset_dir)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-05T19:53:15.063121Z",
          "iopub.execute_input": "2025-03-05T19:53:15.063545Z",
          "iopub.status.idle": "2025-03-05T19:53:44.919733Z",
          "shell.execute_reply.started": "2025-03-05T19:53:15.063509Z",
          "shell.execute_reply": "2025-03-05T19:53:44.918650Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R4D7SoH6p9K",
        "outputId": "de24c52e-0a51-4149-cc07-c85728a6a57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1pt75Fs4APfvxQ1FiuubEWkRJp7tjwndW\n",
            "From (redirected): https://drive.google.com/uc?id=1pt75Fs4APfvxQ1FiuubEWkRJp7tjwndW&confirm=t&uuid=af1f0e93-e1ff-4626-8918-c04c12f73435\n",
            "To: /content/fsdd.zip\n",
            "\r  0% 0.00/17.4M [00:00<?, ?B/s]\r 39% 6.82M/17.4M [00:00<00:00, 60.3MB/s]\r100% 17.4M/17.4M [00:00<00:00, 102MB/s] \n",
            "Dataset directory: /content/fsdd\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kaggle\n",
        "\n",
        "On a Kaggle Notebook, the dataset can be downloaded directly using the Kaggle API through the `kagglehub` package. The cell below downloads the latest version of the dataset from Kaggle and sets the `dataset_dir` variable to the path where the dataset files are located. This provides a quick and efficient method to access the dataset without needing additional steps.\n"
      ],
      "metadata": {
        "id": "Xwg2lKEd6p9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# download latest version of the dataset\n",
        "dataset_dir = kagglehub.dataset_download(\"toribiodiego/free-spoken-digit-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", dataset_dir)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T06:33:27.351992Z",
          "iopub.execute_input": "2025-03-02T06:33:27.352334Z",
          "iopub.status.idle": "2025-03-02T06:33:27.449747Z",
          "shell.execute_reply.started": "2025-03-02T06:33:27.352302Z",
          "shell.execute_reply": "2025-03-02T06:33:27.449119Z"
        },
        "id": "riRjCe296p9M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Introduction and Objectives**\n",
        "\n",
        "### **1.1 Context**\n",
        "Welcome to the audio classification assignment!\n",
        "<br>\n",
        "\n",
        "In this *pre-game assignment*, we’ll work with the [Free-Spoken-Digit Dataset (FSDD)](https://github.com/Jakobovski/free-spoken-digit-dataset)—a collection of short audio recordings of speakers saying a single digit (0–9). Although this dataset is much simpler than real clinical data (like the Alzheimer’s speech recordings we'll be working on), it will provide an introduction to the end-to-end process of building a machine learning model, from working with raw audio to making predictions.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **1.2 Key Terms**\n",
        "\n",
        "Each section in this notebook will include a \"Key Terms\" subsection where we define important concepts from Signal Processing or Machine Learning. The goal is to keep these definitions lean enough to give you a good starting point into exploring the ML space, yet not so exhaustive that they distract from the hands-on experience. If you come across any term or concept that you’re unsure about or for which you do not have a satisfactory explanation, please feel free to reach out for clarification.\n",
        "\n",
        "For practical illustration, we reference the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset—an essential benchmark in machine learning. MNIST consists of 70,000 images of handwritten digits (0–9), with each image being 28x28 pixels. The task is to classify each image into one of 10 classes. This dataset is used to anchor definitions with concrete examples when needed.\n",
        "\n",
        "#### Definitions\n",
        "\n",
        "**Model:**  \n",
        "A model is a mathematical or computational system that learns patterns from data and makes predictions or decisions based on those patterns. In machine learning, a model is trained to capture relationships between inputs (data) and outputs (labels).  \n",
        "- *Example:* A model learns which pixel patterns correspond to each handwritten digit, enabling it to predict the digit in a new image.\n",
        "\n",
        "**Dataset:**  \n",
        "A dataset is a collection of data used for training, validating, and testing machine learning models. It typically consists of pairs of input data (such as images, audio, or text) and their corresponding outputs (labels) that the model learns to predict.  \n",
        "- *Example:* The input data are images of handwritten digits, and each image is paired with a label indicating the digit it represents.\n",
        "\n",
        "**Labels:**  \n",
        "Labels are the specific outputs assigned to individual examples in a dataset. They represent the correct answers or categories for the input data.\n",
        "\n",
        "**Classes:**  \n",
        "Classes refer to the complete set of possible categories into which the examples can be grouped.  \n",
        "- *Example:* Each image is given a label (like \"5\"), and there are 10 possible classes (the digits 0 through 9) covering all the labels.\n",
        "\n",
        "**Supervised Learning:**  \n",
        "Supervised learning involves training a model on a dataset that includes both the inputs and their corresponding labels. The objective is for the model to learn the mapping from inputs to outputs so that it can predict the correct labels for unseen data.  \n",
        "- *Example:* Each image is paired with its label (the correct digit), making it a supervised learning task.\n",
        "\n",
        "**Unsupervised Learning:**  \n",
        "Unsupervised learning is used when the dataset includes only inputs, with no labels provided. The goal is for the model to automatically discover the underlying structure or groupings in the data.  \n",
        "- *Example:* Given a collection of unlabeled handwritten digit images, an unsupervised algorithm (like K-means clustering) might group similar images together, potentially revealing clusters corresponding to different digits.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **1.3 Objective**\n",
        "\n",
        "When you first receive a new dataset, it's essential to understand its structure, contents, and any patterns that could impact model performance. In this exercise, you'll inspect the Free-Spoken-Digit Dataset (**FSDD**), learn how the data is stored, understand the labeling system, and visually examine the dataset distribution. This will help you build good habits for data exploration, ensuring you can confidently begin modeling.\n",
        "\n",
        "You'll primarily use Python's built-in libraries (**`os`**), as well as **`pandas`** and **`matplotlib`** for easy visualization.\n",
        "\n",
        "> **Exercise 1**  \n",
        "> Explore and understand the dataset structure before modeling:\n",
        ">\n",
        "> 1. **Inspect the File Structure**  \n",
        ">       - **List Files:**  \n",
        ">           - Use the **`os`** module to list and print the names of the first 10 `.wav` files from the dataset directory.  \n",
        ">           - Confirm the file format clearly (e.g., `.wav`).  \n",
        ">       - **Count Files:**  \n",
        ">           - Determine and print the total number of `.wav` files to get an initial sense of dataset size.\n",
        "> <br>\n",
        "\n",
        "> 2. **Analyze the Naming Scheme**  \n",
        ">       - **Examine Naming Format:**  \n",
        ">           - Note that each file follows this naming convention:  \n",
        ">             ```digit_speaker_recording-idx.wav```  \n",
        ">               - **Digit (class):** Represents the digit spoken (0–9).  \n",
        ">               - **Speaker:** Indicates the speaker’s identity.  \n",
        ">               - **Recording Index:** Reflects the sequence number of recordings for each speaker-digit pair.  \n",
        ">       - **Interpretation:**  \n",
        ">           - Clearly explain how this naming scheme can help quickly identify important metadata (digit, speaker, recording number) from each audio file.  \n",
        ">           - Briefly discuss the expected distribution of recordings per digit and per speaker based on this naming pattern.\n",
        "> <br>\n",
        "\n",
        "> 3. **Examine and Visualize Dataset Distribution**  \n",
        ">       - **Parse File Names:**  \n",
        ">           - Loop through all filenames and parse them into three separate components (digit, speaker, recording index).  \n",
        ">           - Organize these parsed results into a clear and structured `pandas` DataFrame.  \n",
        ">       - **Distribution Analysis:**  \n",
        ">           - Use the DataFrame to count and summarize the number of recordings available for each digit.  \n",
        ">           - Further group the data to count how many recordings each speaker provided for each digit.  \n",
        ">       - **Visualization:**  \n",
        ">           - Use **`matplotlib`** to plot a clearly labeled stacked bar chart showing the number of recordings per digit, with segments representing each speaker's contribution.  \n",
        ">           - Label your axes clearly (e.g., \"Digit,\" \"Number of Recordings\") and include a legend indicating speakers.\n",
        "> <br>\n",
        "\n",
        "> 3. **Reflect on Your Observations**  \n",
        ">       - After visualizing the data, briefly comment on the balance of the dataset.  \n",
        ">           - Are some digits better represented than others?  \n",
        ">           - Do certain speakers contribute disproportionately more recordings?\n",
        ">           - How might this impact your model’s ability to generalize?\n",
        "\n",
        "**Additional Recommendations:**  \n",
        "- Clearly document each step of your code, especially the parsing and visualization portions.  \n",
        "- Ensure your DataFrame clearly labels columns (e.g., `\"digit\"`, `\"speaker\"`, `\"repetition\"`) to make analysis straightforward.  \n",
        "- Get comfortable using **`pandas`** early, as it’s invaluable for structured data analysis throughout the machine learning pipeline."
      ],
      "metadata": {
        "id": "PSvO-HW-6p9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Inspect the File Structure"
      ],
      "metadata": {
        "id": "D8egv_KX6p9O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T06:30:11.138263Z",
          "iopub.execute_input": "2025-03-02T06:30:11.138542Z",
          "iopub.status.idle": "2025-03-02T06:30:11.153676Z"
        },
        "id": "RhmoNrxO6p9O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Naming Scheme"
      ],
      "metadata": {
        "id": "UOcd5QjU6p9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Dataset Distribution"
      ],
      "metadata": {
        "id": "A9jiO1tQ6p9P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "06sWBvAs6p9P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Data Preprocessing**\n",
        "\n",
        "### **2.1 Working With Audio Data**\n",
        "\n",
        "Before training a model, it is crucial to prepare your audio data so that it is consistent and ready for analysis. In this section, you will learn to load audio files, inspect their waveforms, and extract key properties such as the sample rate. We will use the **`librosa`** library for these tasks.\n",
        "<br>\n",
        "\n",
        "When you load an audio file, you obtain:\n",
        "  - **waveform**: an array of amplitude values over time.\n",
        "  - **sample rate**: the number of audio samples recorded per second.\n",
        "\n",
        "Understanding these properties helps you verify that the recordings are consistent and of good quality for further processing.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **2.2 Key Terms**\n",
        "\n",
        "#### Definitions\n",
        "\n",
        "**Data Processing:**  \n",
        "These are the steps you take to clean and transform raw data into a format that a machine learning model can use effectively.  \n",
        "- *Example:* For MNIST, data processing might involve converting the raw image files into arrays of pixel values that can be fed into a model.\n",
        "\n",
        "**Normalization:**  \n",
        "This means scaling data so that it fits within a specific range, like 0 to 1, which makes it easier to compare features directly.  \n",
        "- *Example:* When working with MNIST, you might scale the pixel values (which originally range from 0 to 255) down to a 0–1 range to help the model learn better.\n",
        "\n",
        "**Standardization:**  \n",
        "This is about adjusting the data so it has a mean of zero and a standard deviation of one. This way, each feature has equal importance during the learning process.  \n",
        "\n",
        "**Truncation:**  \n",
        "Truncation involves cutting off parts of the data to ensure all samples have a consistent size or to remove unnecessary information.  \n",
        "- *Example:* Although MNIST images are already the same size, in other datasets, you might truncate longer audio recordings to keep them at a standard length for processing.\n",
        "\n",
        "**Padding:**  \n",
        "Padding means adding extra values (often zeros) to data samples so that they all match a required size or shape.  \n",
        "- *Example:* If you were dealing with text sequences or variable-sized images, you might add padding to the sequences or images so that every input to the model has the same dimensions.\n",
        "\n",
        "**Preprocessing Pipeline:**  \n",
        "This is a series of data processing steps that are applied in a set order to prepare your entire dataset for modeling.  \n",
        "- *Example:* An MNIST preprocessing pipeline might include converting images to arrays and normalizing the pixel valuesm before they go into the model.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **2.3 Objective**\n",
        "\n",
        "In this section, you'll become familiar with working directly with audio data by loading and inspecting audio files, visualizing waveforms, and analyzing basic audio properties. The goal is to ensure you understand the structure and characteristics of your data before proceeding to modeling. We’ll use **`librosa`** for audio processing, **`matplotlib`** for visualization, and optionally **`tqdm`** to track your progress through the dataset.\n",
        "\n",
        "> **Exercise 2**  \n",
        "> Explore, inspect, and visualize the audio dataset:\n",
        ">  \n",
        "> 1. **Inspect an Audio File**  \n",
        ">       - Use `librosa.load()` to load a single `.wav` audio file from the dataset directory.  \n",
        ">       -  Check and print the shape of the loaded waveform array and the sampling rate (e.g., using `.shape` and by directly printing the sampling rate).  \n",
        ">       -  Plot the waveform clearly labeling the axes (e.g., time in seconds and amplitude). You can use `librosa.display.waveshow()` to simplify plotting.\n",
        "> <br>\n",
        "\n",
        "> 2. **Analyze Audio Properties Across the Entire Dataset**  \n",
        ">       -  Loop through all audio files in the dataset (consider using `tqdm` for a progress bar).  \n",
        ">       - For each audio file, calculate and store:\n",
        ">           - The duration (length of the waveform divided by its sampling rate).\n",
        ">           - The sampling rate.  \n",
        ">       - Create histograms to visualize the distribution of audio durations and sampling rates using `matplotlib`.  \n",
        ">       -  Check and summarize the range (minimum and maximum) of these values.  \n",
        "> <br>\n",
        "\n",
        "> 3. **Standardizing Audio Data:**  \n",
        ">       -  Compute the **global mean** and **global standard deviation** of amplitudes across the entire dataset. (You may use `numpy.concatenate` to merge all waveforms for this calculation.)  \n",
        ">            -  Print these global statistics clearly.  \n",
        ">            -  Standardize a single waveform (for demonstration purposes, choose the first file) using these global mean and standard deviation values.  \n",
        ">            -  Plot a clear comparison between the original waveform and its globally standardized version, labeling the axes and providing a legend to distinguish them.\n",
        "\n",
        "**Additional Tips and Recommendations:**\n",
        "\n",
        "- Consider using the `tqdm` library to display a progress bar when looping through the audio files. This is particularly useful for larger datasets.\n",
        "- Clearly label and format your plots for readability, as these will help you better understand the data and communicate your findings effectively.\n",
        "- Document your code clearly and concisely—remember, your notebook should serve as a clear reference for both yourself and others.  "
      ],
      "metadata": {
        "id": "XgVthQmI6p9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Inspect Audio"
      ],
      "metadata": {
        "id": "2NDeGv3n6p9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "TQPKVSBb6p9R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Audio Properties"
      ],
      "metadata": {
        "id": "D2m-Yra96p9R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "g2Kyywwk6p9R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Standardization"
      ],
      "metadata": {
        "id": "r5RoIFvf6p9S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "RiTuC2qE6p9S"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Feature Extraction**\n",
        "\n",
        "### **3.1 Why Extract Features?**\n",
        "\n",
        "When working with audio data, raw waveforms can be challenging for many machine learning algorithms. Instead of feeding the entire audio signal into a model, we convert each audio file into a set of numerical features that capture important characteristics of the sound—such as pitch, loudness, and frequency content. These features simplify the learning process and can significantly boost model performance.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **3.2 Key Terms**\n",
        "\n",
        "#### Definitions\n",
        "\n",
        "- **Spectrogram:**  \n",
        "  A visual representation of the spectrum of frequencies of an audio signal as it varies with time. Spectrograms provide intuitive visual insights into audio signals by clearly illustrating how frequency content changes dynamically.\n",
        "  - *Example:* In speech recognition, spectrograms show distinct patterns corresponding to different spoken digits or phonemes.\n",
        "\n",
        "**Windowing:**  \n",
        "  A technique used in audio processing where signals are divided into short segments (windows) for localized analysis. Commonly used when computing MFCCs or other spectral features.  \n",
        "  - *Example:* Computing MFCCs typically involves splitting audio signals into overlapping frames to capture changes over time.\n",
        "\n",
        "**Feature Extraction:**  \n",
        "The process of converting raw audio data into numerical descriptors (features) that capture the important properties of the sound. These features are used as inputs to machine learning models.  \n",
        "- *Example:* When analyzing a spoken word clip, you might extract features like pitch, tone, and rhythm so the model can learn to recognize different words.\n",
        "\n",
        "**MFCCs (Mel-Frequency Cepstral Coefficients):**  \n",
        "A popular set of features in speech and audio analysis that approximate how humans perceive sound by emphasizing critical frequency ranges.  \n",
        "- *Example:* For a recording of someone saying \"hello,\" MFCCs help capture the unique frequency patterns that distinguish that word from others.\n",
        "\n",
        "**eGeMAPS:**  \n",
        "A standardized set of features designed for analyzing emotional and paralinguistic aspects of speech. It includes various frequency, energy, and spectral parameters.  \n",
        "- *Example:* In an emotion recognition task, eGeMAPS can help quantify subtle changes in a speaker's voice that might indicate happiness, sadness, or stress.\n",
        "\n",
        "**Audio Feature Vector:**  \n",
        "A numerical representation of an audio clip, created by combining various extracted features.  \n",
        "- *Example:* If you extract 13 MFCC coefficients along with additional features like energy and pitch, you might end up with a vector of 20 or more values that summarizes the audio clip’s characteristics.\n",
        "\n",
        "**Feature Extraction Pipeline:**  \n",
        "The sequence of steps you follow to compute and save features from raw audio data. Once extracted, these features are often saved (for example, as CSV files or NumPy arrays) so that you can quickly load them for future experiments without needing to reprocess the audio each time.  \n",
        "- *Example:* In a project analyzing a collection of interview recordings, you might set up a pipeline that automatically processes each audio file, extracts MFCCs and eGeMAPS features, and then stores them for quick access during model training.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **3.3 Objective**\n",
        "\n",
        "In this section, you’ll extract meaningful audio features from raw audio data to make them suitable inputs for machine learning models. You'll learn how to compute widely used audio features such as MFCCs and eGeMAPS. Additionally, you'll practice saving and loading processed data, which is crucial for efficient workflows.\n",
        "\n",
        "We’ll use **`librosa`** for MFCC extraction, **`openSMILE`** for extracting eGeMAPS features, and **`matplotlib`** for visualizations. You'll also learn to manage datasets efficiently by saving and reloading processed data.\n",
        "\n",
        "> **Exercise 3**  \n",
        "> Extract, visualize, and store audio features from your dataset:\n",
        ">  \n",
        "> **1. Extract and Visualize Spectrogram and MFCC Features**  \n",
        "> - Load an audio file using `librosa.load()`.  \n",
        "> - Compute and visualize a **spectrogram** of the audio signal.  \n",
        ">   - Convert the spectrogram to the decibel (dB) scale for better visualization clarity.  \n",
        ">   - Limit the displayed frequencies to **4000 Hz** to make important audio characteristics clearer.  \n",
        ">   - Ensure your plot has labeled axes indicating frequency (Hz), time (seconds), and intensity (dB).\n",
        "> - Extract **MFCC features** (use 13 coefficients) from the audio file.  \n",
        ">   - Print the shape of the MFCC feature array clearly.  \n",
        ">   - Visualize the MFCC features using `librosa.display.specshow()`.  \n",
        "> <br>\n",
        "\n",
        "> 2. **Extract eGeMAPS Features Using openSMILE**  \n",
        ">       - Use the **openSMILE** toolkit to extract eGeMAPS features from a single audio file (this involves calling openSMILE via Python).  \n",
        ">       - Clearly print the shape of the extracted feature array and briefly comment on what the shape implies about the number of features extracted.  \n",
        ">       - (Hint: You may define a helper function in Python to simplify feature extraction from openSMILE’s output format.)  \n",
        "> <br>\n",
        "\n",
        "> 3. **Saving and Reloading Features**  \n",
        ">       - After extracting the eGeMAPS features, save them to disk using a convenient file format such as CSV or NumPy binary (`.npy`) format. (You might use Python’s built-in file I/O or NumPy’s saving functionality.)  \n",
        ">       - Reload the features from the saved file and print their shape to verify they match the original features exactly. This practice helps you avoid repeatedly extracting features, especially from large datasets.  \n",
        "> <br>\n",
        "\n",
        "> 4. **Prepare the Entire Dataset for Modeling**  \n",
        ">       - Loop through all audio files in your dataset (consider using **`tqdm`** to display a progress bar).  \n",
        ">           - For each file:\n",
        ">               - Extract the raw eGeMAPS features.\n",
        ">               - Store the associated digit label.\n",
        ">       - Compute global statistics (mean and standard deviation) across all extracted eGeMAPS features. (You may find `numpy.concatenate()` helpful here.)  \n",
        ">       - Using these global statistics, create three different versions of your dataset features for further experimentation:\n",
        ">           - **Raw:** Original extracted features with no modifications.\n",
        ">           - **Standardized:** Features standardized using the global mean and standard deviation.\n",
        ">           - **Standardized + Noise:** Features that have been standardized and then had small Gaussian noise added (useful for testing robustness).  \n",
        "> <br>\n",
        "\n",
        "> 5. **Saving Processed Feature Sets**  \n",
        ">       - Save each of the three feature sets separately, clearly labeling them (e.g., `egemaps_raw.npy`, `egemaps_std.npy`, `egemaps_std_noise.npy`).  \n",
        ">       - Also save the labels array separately for easy reference in future modeling tasks.\n",
        ">       - After saving, load each feature set again to verify the saved data matches the original feature arrays. Print out their shapes to confirm successful saving and loading.\n",
        "\n",
        "**Additional Guidance:**  \n",
        "- Clearly document your functions, particularly when integrating external tools like openSMILE.  \n",
        "- Verify your results step-by-step to catch any issues early on.  \n",
        "- If you encounter difficulties with openSMILE, ensure it is installed and configured correctly—feel free to ask if you're uncertain about setup.  \n"
      ],
      "metadata": {
        "id": "04t4k7Vl6p9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Extract MFCCs"
      ],
      "metadata": {
        "id": "ng9_8_tG6p9T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T06:33:40.769513Z",
          "iopub.execute_input": "2025-03-02T06:33:40.769802Z",
          "iopub.status.idle": "2025-03-02T06:33:40.977107Z",
          "shell.execute_reply.started": "2025-03-02T06:33:40.769781Z",
          "shell.execute_reply": "2025-03-02T06:33:40.976305Z"
        },
        "id": "Bq6yf9Fa6p9T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Extract eGeMAPS Features"
      ],
      "metadata": {
        "id": "r6S3myJ56p9U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T06:33:43.792943Z",
          "iopub.execute_input": "2025-03-02T06:33:43.793226Z",
          "iopub.status.idle": "2025-03-02T06:33:43.849434Z",
          "shell.execute_reply.started": "2025-03-02T06:33:43.793205Z",
          "shell.execute_reply": "2025-03-02T06:33:43.848536Z"
        },
        "id": "e9crZICG6p9U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Save and Reload Features"
      ],
      "metadata": {
        "id": "koxzBk3V6p9U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "poWcHawF6p9U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Extract Feature for the Entire Dataset"
      ],
      "metadata": {
        "id": "Mf3h53uw6p9V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T06:33:48.203442Z",
          "iopub.execute_input": "2025-03-02T06:33:48.203743Z",
          "iopub.status.idle": "2025-03-02T06:35:43.266339Z",
          "shell.execute_reply.started": "2025-03-02T06:33:48.203722Z",
          "shell.execute_reply": "2025-03-02T06:35:43.265451Z"
        },
        "id": "-dmlZ51J6p9W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Building and Training Classifiers**\n",
        "\n",
        "\n",
        "## **4. Building and Training Classifiers**\n",
        "\n",
        "### **4.1 What is a Classifier?**\n",
        "\n",
        "A **classifier** is a supervised machine learning algorithm that maps input features—such as MFCCs, spectral features, or other audio-derived numerical descriptors—to discrete output classes (e.g., the spoken digit \"3\"). Mathematically, it approximates a function:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = y,\n",
        "$$\n",
        "\n",
        "where $\\mathbf{x}$ represents a feature vector extracted from the input data and $y$ is the predicted class label.\n",
        "\n",
        "Classifiers learn by minimizing a **loss function**, which measures the difference between the predicted labels and the true labels in the training data. Recall the **Mean Squared Error (MSE)** from your Stochastic Processes class, defined as:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2,\n",
        "$$\n",
        "\n",
        "where $y_i$ is the actual value and $\\hat{y}_i$ is the predicted value. Although MSE is commonly used in regression problems, classification tasks often use loss functions like **cross-entropy** or **log loss** to guide the training process toward accurate predictions.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **4.2 Key Terms**\n",
        "\n",
        "#### Definitions\n",
        "\n",
        "**Classifier:**  \n",
        "A model that assigns input data to predefined categories or classes.  \n",
        "- *Example:* An email filter that classifies incoming emails as either \"spam\" or \"not spam.\"\n",
        "\n",
        "**Supervised Learning:**  \n",
        "A type of machine learning where the model learns from labeled data (data paired with known outcomes). The goal is for the model to learn the mapping from inputs to outputs to correctly predict new, unseen data.  \n",
        "- *Example:* Predicting handwritten digit labels given images labeled with the correct digits.\n",
        "\n",
        "**Training:**  \n",
        "The process of providing labeled examples to a classifier so it can learn the relationships between input features and output labels.  \n",
        "- *Example:* Training a speech recognition model with audio features labeled by spoken words.\n",
        "\n",
        "**Testing:**  \n",
        "Evaluating the performance of a trained model on a set of examples that it has not seen before.  \n",
        "- *Example:* After training a digit classifier, testing involves evaluating its accuracy on a new set of digit recordings.\n",
        "\n",
        "**Generalization:**  \n",
        "The ability of a classifier to perform accurately on new, unseen data rather than only the data it was trained on. Good generalization means the model captures underlying patterns instead of memorizing specific training examples.  \n",
        "- *Example:* A well-generalizing spam classifier correctly identifies spam emails it has never encountered before, not just those in the training dataset.\n",
        "\n",
        "**Decision Boundary:**  \n",
        "  A visual boundary in feature space that separates predicted classes, illustrating how a classifier partitions data.  \n",
        "  - *Example:* A decision boundary of logistic regression is a straight line (in 2D space), clearly separating two classes.\n",
        "\n",
        "**Loss Function:**  \n",
        "A mathematical function measuring how far the model’s predictions are from the true labels. Minimizing the loss guides the training process.  \n",
        "- *Example:* **Cross-Entropy Loss**, commonly used in classification, measures how well the predicted probabilities match the actual class labels.\n",
        "\n",
        "**Cross-Entropy Loss (Log Loss):**\n",
        "  A loss function commonly used in classification tasks, measuring how well predicted probabilities align with actual class labels.  \n",
        "  $$\n",
        "  \\text{Cross-Entropy} = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i)]\n",
        "  $$\n",
        "  - *Example:* When training a digit classifier, cross-entropy loss penalizes incorrect probability assignments, guiding the model toward more confident, accurate predictions.\n",
        "\n",
        "**Gradient:**  \n",
        "A vector of partial derivatives indicating the direction and magnitude in which model parameters must be adjusted to minimize the loss function.  \n",
        "- *Example:* In neural network training, gradients inform how weights should be updated to reduce prediction error iteratively.\n",
        "\n",
        "**Gradient Descent:**  \n",
        "An optimization algorithm that iteratively adjusts model parameters in the direction that reduces the loss function most rapidly (direction of steepest descent).  \n",
        "- *Example:* Adjusting neural network weights repeatedly to minimize classification errors.\n",
        "\n",
        "**Local Minimum:**  \n",
        "A point in parameter space where the loss function has a lower value compared to its immediate surroundings, but not necessarily the lowest possible value overall.  \n",
        "- *Example:* A neural network training process might stop improving prematurely by settling into a local minimum.\n",
        "\n",
        "**Global Minimum:**  \n",
        "The point in parameter space that yields the absolute lowest loss value, representing optimal model performance.  \n",
        "- *Example:* The ideal scenario where a model achieves the lowest possible prediction error across all possible parameter values.\n",
        "\n",
        "**Overfitting:**  \n",
        "When a model performs exceptionally well on training data but poorly on unseen data, indicating that it has learned noise or overly specific details rather than generalizable patterns.  \n",
        "- *Example:* A decision tree that perfectly memorizes the training set but misclassifies new examples is overfitting.\n",
        "\n",
        "**Underfitting:**  \n",
        "When a model fails to adequately capture the underlying patterns in the data, resulting in poor performance on both training and test data.  \n",
        "- *Example:* A linear classifier unable to correctly separate complex, non-linear classes clearly, resulting in low accuracy.\n",
        "\n",
        "**Regularization:**  \n",
        "Techniques used to reduce overfitting by adding constraints or penalties that encourage simpler models.  \n",
        "- *Example:* L2 regularization penalizes large coefficients in logistic regression, leading to more robust and generalized predictions.\n",
        "\n",
        "**Bias-Variance Trade-off:**\n",
        "  The trade-off between a model being too simple (high bias, underfitting) and too complex (high variance, overfitting).  \n",
        "  - *Example:* A shallow decision tree may underfit (high bias), while a very deep decision tree may overfit (high variance).\n",
        "\n",
        "\n",
        "**Accuracy:**  \n",
        "The proportion of correct predictions made by the classifier. Defined mathematically as:  \n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$  \n",
        "- *Example:* If a classifier correctly identifies 95 out of 100 audio samples, it achieves 95% accuracy.\n",
        "\n",
        "**Confusion Matrix:**  \n",
        "A table summarizing a classifier’s performance, showing correct and incorrect predictions explicitly across classes.\n",
        "\n",
        "|                       | **Predicted Positive** | **Predicted Negative** |\n",
        "|-----------------------|------------------------|------------------------|\n",
        "| **Actual Positive**   | TP (True Positive)     | FN (False Negative)    |\n",
        "| **Actual Negative**   | FP (False Positive)    | TN (True Negative)     |\n",
        "\n",
        "**Precision and Recall:**  \n",
        "- **Precision** measures the accuracy of the positive predictions:\n",
        "  $$\n",
        "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "  $$\n",
        "- **Recall** measures the ability to correctly detect positive instances:\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "**F1 Score:**  \n",
        "The harmonic mean of precision and recall, providing a balance between the two metrics:  \n",
        "$$\n",
        "\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$  \n",
        "- *Example:* Used in medical diagnostics to balance correctly detecting disease (high recall) against false alarms (precision).\n",
        "\n",
        "**Confusion Matrix Analysis:**  \n",
        "Examining the confusion matrix in detail to understand the nature and distribution of classification errors. Helpful in diagnosing which classes the model confuses frequently.\n",
        "\n",
        "**Decision Boundary Visualization:**  \n",
        "Graphical visualization of boundaries separating predicted classes. Useful for intuitively understanding how a classifier makes decisions based on features.\n",
        "\n",
        "**Regularization:**  \n",
        "A method to prevent overfitting by adding penalties to complex models. Examples include L1 and L2 regularization.  \n",
        "- *Example:* Penalizing large model parameters (weights) to encourage simpler models.\n",
        "\n",
        "**Hyperparameters:**  \n",
        "Values chosen by the practitioner before training, which govern the learning process, such as the learning rate or number of trees. Unlike model parameters, hyperparameters are not learned from data.\n",
        "\n",
        "**Expert Systems:**  \n",
        "Models that use explicitly defined, handcrafted rules by human experts rather than learned rules from data.  \n",
        "- *Example:* A medical diagnosis tool that uses a specific temperature threshold to define a fever.\n",
        "\n",
        "**Learned Systems:**  \n",
        "Models that learn rules and patterns directly from data rather than predefined rules.  \n",
        "- *Example:* A neural network trained to classify images into different animal species.\n",
        "\n",
        "**Function Approximator:**  \n",
        "A model that learns to map input features to outputs by approximating the underlying relationship.  \n",
        "- *Example:* A classifier that maps audio features to spoken digits.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **On Classifiers**\n",
        "\n",
        "Below are foundational classifiers, each with unique strengths, weaknesses, and suitable application scenarios:\n",
        "\n",
        "**Logistic Regression:**  \n",
        "A linear model estimating the probability of class membership using a logistic function, effective when class boundaries are approximately linear.  \n",
        "- *Strengths:* Fast, interpretable, effective baseline.  \n",
        "- *Weaknesses:* Limited when relationships are complex or nonlinear.  \n",
        "- *Example:* Predicting student pass/fail based on study hours and attendance.\n",
        "\n",
        "**Decision Tree:**  \n",
        "Partitions data into subsets based on feature thresholds, forming a branching structure.  \n",
        "- *Strengths:* Intuitive, easy to interpret and visualize, handles nonlinearity.  \n",
        "- *Weaknesses:* Prone to overfitting, unstable with slight changes in data.  \n",
        "- *Example:* Classifying loan approval by sequentially checking credit score and income levels.\n",
        "\n",
        "**Random Forest:**  \n",
        "An ensemble of many decision trees, each trained on random subsets of data and features, combining their predictions for improved accuracy and reduced overfitting.  \n",
        "- *Strengths:* Robust to noise, reduces variance of single-tree models.  \n",
        "- *Weaknesses:* Less interpretable, computationally heavier than a single decision tree.  \n",
        "- *Example:* Accurately diagnosing diseases by aggregating multiple decision-tree predictions.\n",
        "\n",
        "**XGBoost (Extreme Gradient Boosting):**\n",
        "A powerful ensemble method building trees sequentially, each new tree correcting errors of previous trees. Utilizes gradient boosting, which minimizes the loss function using gradients.  \n",
        "- *Strengths:* Excellent performance, handles nonlinear relationships effectively, popular in competitions due to predictive power.  \n",
        "- *Weaknesses:* Computationally intensive, complex hyperparameter tuning required.  \n",
        "- *Example:* Predicting house prices accurately by iteratively refining predictions based on previous errors.\n",
        "\n",
        "**Multi-Layer Perceptron (MLP; Neural Networks):**  \n",
        "Consists of interconnected layers of neurons (nodes), using nonlinear activation functions to model complex relationships. Training occurs via gradient descent and backpropagation (adjusting weights to minimize loss).  \n",
        "- *Strengths:* Handles extremely complex, nonlinear problems.  \n",
        "- *Weaknesses:* Prone to local minima, requires careful hyperparameter tuning, data-hungry.  \n",
        "- *Example:* Recognizing handwritten digits from pixel patterns, speech recognition tasks, and complex classification problems.\n",
        "\n",
        "**Ensemble Methods:**  \n",
        "Combine predictions from multiple models to improve accuracy and robustness. Common methods include Random Forests, Gradient Boosting, and voting ensembles.  \n",
        "- *Strengths:* Improved accuracy, reduced overfitting.  \n",
        "- *Weaknesses:* Less interpretable, increased complexity.  \n",
        "- *Example:* Combining several weather prediction models to achieve highly accurate forecasts.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Libraries you'll frequently use:**  \n",
        "- **`scikit-learn`** (for Logistic Regression, Decision Trees, Random Forests, MLP)  \n",
        "- **`XGBoost`** (optimized gradient boosting implementation)  \n",
        "- **`TensorFlow` or `PyTorch`** (for advanced neural networks beyond basic MLP).\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "##### **Additional Notes on Classifiers**\n",
        "In this section, we've introduced several common classifiers, mainly through practical examples and general explanations. However, it's important to keep in mind that these models aren't just black-box tools you call from libraries like `sklearn`. Each classifier is built around specific mathematical principles, and each comes with its own strengths, weaknesses, and appropriate use cases. Understanding these differences—even at a high level—can greatly enhance your intuition and your ability to choose the right classifier for your data and task.\n",
        "\n",
        "The descriptions provided above are intentionally simplified to give you an accessible starting point. If you're curious about the deeper theoretical aspects—such as how decision boundaries are formed, the math behind optimization algorithms like gradient descent, the specific loss functions that classifiers minimize, or how concepts like overfitting and regularization shape model performance—I'll be creating additional notes diving into these topics.\n",
        "\n",
        "These supplementary notes aren't required reading, but they'll provide valuable insights if you want to gain a more thorough understanding of how these classifiers actually work under the hood.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **4.3 Objective**\n",
        "\n",
        "In this section, you'll build and train multiple classifiers using the features extracted from your audio dataset. This exercise will give you hands-on experience with training, evaluating, and comparing classifiers. Additionally, you'll visualize and summarize model performance clearly, becoming familiar with helpful Python libraries like **`scikit-learn`**, **`tabulate`**, and visualization tools like **`matplotlib`** and **`seaborn`**.\n",
        "\n",
        "> **Exercise 4**  \n",
        "> Train, evaluate, and compare classifiers to recognize spoken digits using different feature preprocessing approaches:\n",
        ">\n",
        "> 1. **Train Multiple Classifiers**  \n",
        ">       - Load the three variants of saved eGeMAPS features (`raw`, `standardized`, and `standardized+noise`) as NumPy arrays (using `np.load()`).\n",
        ">       - Load the corresponding labels.\n",
        ">       - Split each feature set into training and test sets. Consider using `train_test_split` from `sklearn` with a fixed random seed (`random_state=42`) for reproducibility.\n",
        ">       - Define and train five classifiers from **`sklearn`**:\n",
        ">           - Logistic Regression (`LogisticRegression`)\n",
        ">           - Decision Tree (`DecisionTreeClassifier`)\n",
        ">           - Random Forest (`RandomForestClassifier`)\n",
        ">           - XGBoost (`XGBClassifier`)\n",
        ">           - Multi-Layer Perceptron (MLP) (`MLPClassifier`)\n",
        ">       - Store your trained classifiers in an organized dictionary for later reference.\n",
        "> <br>\n",
        "\n",
        "> 2. **Evaluate and Visualize Classifier Accuracy**  \n",
        ">       - For each trained model and preprocessing variant, compute predictions using the `.predict()` method on the test data.\n",
        ">       - Calculate and record the accuracy for each classifier and preprocessing variant.\n",
        ">       - Using `matplotlib`, create a clear grouped bar chart to visually compare the accuracy scores across the three preprocessing variants (Raw, Standardized, Standardized+Noise).  \n",
        ">            - Clearly label your axes and provide a legend to distinguish preprocessing methods.\n",
        "> <br>\n",
        "\n",
        "> 3. **Summarize Model Performance Clearly (Using Tabulate)**  \n",
        ">       - Use the `tabulate` package to create an organized table summarizing accuracy for each classifier (rows) across all three preprocessing variants (columns).  \n",
        ">            - Provide descriptive headers and use the `\"github\"` format to enhance readability.\n",
        ">       - Clearly indicate which combination of classifier and preprocessing variant achieved the highest accuracy.\n",
        "> <br>\n",
        "\n",
        "> 4. **Confusion Matrix Analysis (Detailed Evaluation)**  \n",
        ">       - Based on your best preprocessing variant, select the corresponding test dataset.  \n",
        ">       - Use your best-performing model to predict labels on this test set and calculate the confusion matrix using `confusion_matrix()` from `sklearn.metrics`.\n",
        ">       - Visualize the confusion matrix using `seaborn` to plot a heatmap clearly labeling axes with class labels.  \n",
        ">           - Ensure the heatmap displays clear annotations indicating correct and incorrect predictions.\n",
        "> <br>\n",
        "\n",
        "**Additional Guidance:**  \n",
        "- Use clear variable naming conventions to keep track of your data, models, and preprocessing methods.\n",
        "- Consider using the **`tqdm`** library to monitor the training progress of your models, particularly helpful when training complex models or larger datasets.\n",
        "- Make sure to import and document clearly which libraries you’re using (`numpy`, `sklearn`, `xgboost`, `matplotlib`, `seaborn`, `tabulate`, etc.) in your notebook.  \n",
        "- Document each step of your workflow clearly so that you (and your peers) can easily revisit your work later or replicate your results."
      ],
      "metadata": {
        "id": "2xDIqwP16p9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Train Multiple Classifiers"
      ],
      "metadata": {
        "id": "fa7HTuh66p9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T06:36:26.714831Z",
          "iopub.execute_input": "2025-03-02T06:36:26.715125Z",
          "iopub.status.idle": "2025-03-02T06:36:43.797556Z",
          "shell.execute_reply.started": "2025-03-02T06:36:26.715103Z",
          "shell.execute_reply": "2025-03-02T06:36:43.796626Z"
        },
        "id": "GaN1rqp16p9Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Evalaute Model Performance"
      ],
      "metadata": {
        "id": "cPbA6eS_6p9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T06:36:43.798793Z",
          "iopub.execute_input": "2025-03-02T06:36:43.799075Z",
          "iopub.status.idle": "2025-03-02T06:36:44.004153Z",
          "shell.execute_reply.started": "2025-03-02T06:36:43.799053Z",
          "shell.execute_reply": "2025-03-02T06:36:44.003335Z"
        },
        "id": "Mhv3u07t6p9Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Summarize Performance"
      ],
      "metadata": {
        "id": "Ki82pzu_6p9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T06:36:51.131538Z",
          "iopub.execute_input": "2025-03-02T06:36:51.131822Z",
          "iopub.status.idle": "2025-03-02T06:36:51.139705Z",
          "shell.execute_reply.started": "2025-03-02T06:36:51.131801Z",
          "shell.execute_reply": "2025-03-02T06:36:51.138688Z"
        },
        "id": "9gQXQhCh6p9Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Confusion Matrix"
      ],
      "metadata": {
        "id": "LBFzu3a46p9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T06:36:56.015122Z",
          "iopub.execute_input": "2025-03-02T06:36:56.015456Z",
          "iopub.status.idle": "2025-03-02T06:36:56.450091Z",
          "shell.execute_reply.started": "2025-03-02T06:36:56.015426Z",
          "shell.execute_reply": "2025-03-02T06:36:56.449255Z"
        },
        "id": "6CPlpKGc6p9Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Feature Importance and Selection**\n",
        "\n",
        "### **5.1 Introducing Feature Importance and Selection**\n",
        "\n",
        "Feature importance measures how much each individual feature contributes to a model’s predictions. By understanding which features are most influential, you can interpret your model better and simplify it by removing redundant or noisy features. This process—known as feature selection—can lead to models that generalize better, train faster, and are easier to explain.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **5.2 Key Terms**\n",
        "#### Definitions\n",
        "\n",
        "**Feature Importance:**  \n",
        "A metric that quantifies how significantly each feature (such as MFCC coefficients or spectral features) influences a model’s predictions.  \n",
        "- *Example:* In a speech recognition model, feature importance might reveal that certain frequency bands are crucial for distinguishing vowels from consonants.\n",
        "\n",
        "**Tree-Based Feature Importance:**  \n",
        "An approach used by tree-based models (like Random Forests or XGBoost), which assigns importance scores based on how much each feature helps reduce uncertainty or impurity during training.  \n",
        "- *Example:* In classifying weather conditions, a Random Forest might indicate temperature and humidity as the most important features, as these significantly reduce prediction uncertainty.\n",
        "\n",
        "**Permutation Importance:**  \n",
        "A model-agnostic approach to determining feature importance. It involves randomly shuffling feature values and measuring how much this impacts model accuracy—large drops imply high importance.  \n",
        "- *Example:* In detecting fraud, shuffling the \"transaction amount\" feature values might substantially lower accuracy, indicating that this feature strongly influences model predictions.\n",
        "\n",
        "**Feature Selection:**  \n",
        "The process of identifying and retaining only the most relevant features from the full feature set to improve model performance, reduce training time, and simplify interpretation.  \n",
        "- *Example:* For disease prediction, selecting only the most predictive medical indicators (e.g., specific biomarkers) from numerous tests.\n",
        "\n",
        "**Filter Methods:**  \n",
        "Simple and computationally efficient feature selection techniques that rank features based on statistical metrics (such as correlation or mutual information) independent of any classifier.  \n",
        "- *Example:* Selecting the top 10 features with the highest correlation to the target class, irrespective of the model used.\n",
        "\n",
        "**Wrapper Methods:**  \n",
        "Iterative techniques that select features based on model performance. They repeatedly train models, removing the least important features at each iteration (e.g., Recursive Feature Elimination).  \n",
        "- *Example:* Using Recursive Feature Elimination (RFE) to iteratively remove irrelevant words from a text classification task, identifying a minimal, highly predictive set of words.\n",
        "\n",
        "**Embedded Methods:**  \n",
        "Feature selection methods integrated directly into the model training process. These methods automatically determine the importance of features as the model learns, often using regularization techniques.  \n",
        "- *Example:* Using Lasso regression (L1 regularization), which shrinks the coefficients of less relevant features toward zero, effectively selecting features during training.\n",
        "\n",
        "\n",
        "**Top-K Feature Selection:**  \n",
        "Selecting the K most important features based on predefined importance metrics or scores, simplifying the model and potentially improving efficiency.  \n",
        "- *Example:* From 100 audio-derived features, you might select only the top 10 most influential to simplify the model and maintain good performance.\n",
        "\n",
        "**Cross-Validation:**  \n",
        "A validation technique where the dataset is divided into subsets (folds). The model is trained and evaluated multiple times using different folds, improving reliability of performance estimates.  \n",
        "- *Example:* In 5-fold cross-validation, data is split into five subsets; the model is trained on four subsets and evaluated on the remaining one. This process repeats five times, ensuring robust validation.\n",
        "\n",
        "**Multicollinearity:**  \n",
        "  Occurs when two or more features are highly correlated with each other, potentially affecting feature importance interpretation and model stability.  \n",
        "  - *Example:* If \"age\" and \"experience\" in job applicants are highly correlated, one might overshadow the importance of the other.\n",
        "\n",
        "**Redundant Features:**  \n",
        "  Features that provide no new information because their information is captured entirely by other features.  \n",
        "  - *Example:* Including both temperature in Celsius and Fahrenheit as separate features provides no new predictive information.\n",
        "\n",
        "**Irrelevant (Noisy) Features:**  \n",
        "  Features that have little to no predictive power regarding the target outcome.  \n",
        "  - *Example:* In predicting home prices, the color of the mailbox is likely irrelevant and should be removed.\n",
        "\n",
        "**Curse of Dimensionality:**  \n",
        "  Phenomenon where having too many features relative to the number of training examples can degrade model performance, often requiring feature selection or dimensionality reduction.  \n",
        "  - *Example:* Classifying diseases based on thousands of genes with only dozens of patients may lead to poor performance without feature selection.\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **5.3 Objective**\n",
        "\n",
        "In this exercise, you'll explore **feature importance and feature selection**, learning how to identify the most influential features for your model’s predictions. You’ll practice assessing the importance of each feature, simplifying your dataset based on this importance, and evaluating the impact of feature selection on classifier performance and efficiency. This is crucial for developing simpler, faster, and potentially more accurate machine learning models.\n",
        "\n",
        "You'll primarily use the **Random Forest classifier** from **`scikit-learn`** because it provides intuitive and reliable feature importance scores.\n",
        "\n",
        "> **Exercise 5**  \n",
        "> Explore feature importance and apply feature selection techniques:\n",
        ">  \n",
        "> 1. **Assess Feature Importance Using Random Forest**  \n",
        ">       - Train a Random Forest classifier (`RandomForestClassifier`) using your standardized feature set.  \n",
        ">       - After training, extract and visualize feature importance scores clearly using `matplotlib`. Label your plot carefully, with axes titles (e.g., \"Feature Index\" and \"Importance Score (%)\").  \n",
        ">       - To make interpretation easier, consider plotting importance scores as percentages.  \n",
        ">       - Clearly label your plot axes and title your visualization to ensure clarity.\n",
        "> <br>\n",
        "\n",
        "> 2. **Identify and Summarize the Top Features**  \n",
        ">       - Using the feature importance scores, identify and clearly list the top **10 most important features**.  \n",
        ">       - Summarize these top features in a table using the **`tabulate`** package (recommended table format: `\"github\"` style).  \n",
        ">            - Include columns such as \"Rank,\" \"Feature Index,\" \"Feature Name,\" and \"Importance Score (%)\".\n",
        "> <br>\n",
        "\n",
        "> 3. **Evaluate Classifier Performance After Feature Selection**  \n",
        ">       - Perform feature selection by removing the least important features incrementally. Consider removing 20%, 50%, 70%, and 90% of features based on their importance rankings.\n",
        ">           - For each reduction level:\n",
        ">               - Retrain the Random Forest classifier on the reduced feature set.\n",
        ">               - Record and summarize:\n",
        ">                   - The number of features kept.\n",
        ">                   - The new accuracy score on the test set.\n",
        ">                   - The training time required (use Python’s `time` module to track this).\n",
        ">       - Summarize these results in a clearly formatted table using the **`tabulate`** package.  \n",
        "> <br>\n",
        "\n",
        "> 4. **Visualize the Impact of Feature Reduction**  \n",
        ">       - Create a bar chart with `matplotlib` to visualize the impact of feature reduction on model accuracy.\n",
        ">           - Clearly label the x-axis (percentage of features removed) and the y-axis (accuracy).\n",
        ">           - Consider using distinct colors to enhance readability.\n",
        "> <br>\n",
        "\n",
        "> 5. **Reflect on Your Results**  \n",
        ">       - Clearly summarize your observations:  \n",
        ">           - Did reducing the number of features significantly affect model performance (accuracy)?\n",
        ">           - Did training time decrease significantly with fewer features?\n",
        ">           - Provide a brief discussion on why certain features might be more important and how removing less relevant features can influence the model’s performance and efficiency.\n",
        "\n",
        "**Additional Recommendations:**  \n",
        "- Use the **`tqdm`** library when retraining models on reduced feature sets to clearly track your progress, especially if you experiment with several reduction levels.\n",
        "- Clearly comment your code, explaining each step—this will help you and your peers understand each phase of your analysis.  \n",
        "- Document your findings briefly but clearly, explaining how the insights gained could inform future modeling decisions.\n"
      ],
      "metadata": {
        "id": "jpeOfQul6p9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Assess Feature Importance Using Random Forest"
      ],
      "metadata": {
        "id": "GLzbN0GI6p9a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T07:07:55.092187Z",
          "iopub.execute_input": "2025-03-02T07:07:55.092527Z",
          "iopub.status.idle": "2025-03-02T07:07:56.773750Z",
          "shell.execute_reply.started": "2025-03-02T07:07:55.092497Z",
          "shell.execute_reply": "2025-03-02T07:07:56.772778Z"
        },
        "id": "hlR5jq7a6p9a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Feature Selection and Model Comparison"
      ],
      "metadata": {
        "id": "aVGa63EA6p9a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T07:09:29.469750Z",
          "iopub.execute_input": "2025-03-02T07:09:29.470046Z",
          "iopub.status.idle": "2025-03-02T07:09:35.126636Z",
          "shell.execute_reply.started": "2025-03-02T07:09:29.470023Z",
          "shell.execute_reply": "2025-03-02T07:09:35.125712Z"
        },
        "id": "gilzgXR96p9a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Reflect on Results"
      ],
      "metadata": {
        "id": "reuuwKAP6p9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Hyperparameter Tuning**\n",
        "\n",
        "### **6.1 Introduction to Hyperparameter Tuning**\n",
        "\n",
        "When training a model, not all settings are learned from the data. Hyperparameters are values set before training begins that control how the model learns and its overall complexity. Choosing the right hyperparameters can have a dramatic effect on performance and generalization. In this section, you will learn how to systematically search for optimal hyperparameter values to improve your classifier’s accuracy.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **6.2 Key Terms**\n",
        "\n",
        "#### Definitions\n",
        "\n",
        "**Model Parameters:**  \n",
        "These are the values a model learns during training, such as the weights in a neural network.  \n",
        "- *Example:* In a neural network, parameters are adjusted during training to minimize errors between predictions and actual values.\n",
        "\n",
        "**Hyperparameters:**  \n",
        "These are the values set before training that control the learning process and the model’s complexity, such as the number of trees in a Random Forest or the learning rate in a neural network.  \n",
        "- *Example:* When training a neural network, you might set the learning rate and the number of hidden layers as hyperparameters.\n",
        "\n",
        "**Grid Search:**  \n",
        "An exhaustive search method that tests every combination of specified hyperparameter values to find the best configuration.  \n",
        "- *Example:* In tuning a model, grid search might try every combination of learning rates and batch sizes to determine which setup gives the best validation performance.\n",
        "\n",
        "**Random Search:**  \n",
        "A method that tests random combinations of hyperparameters, which can be faster than grid search when the search space is large.  \n",
        "- *Example:* Instead of testing every possible combination, random search might select a random set of hyperparameters, allowing you to quickly identify promising configurations.\n",
        "\n",
        "**Cross-Validation:**  \n",
        "A technique used to assess how well a model will generalize to an independent dataset by dividing the data into several folds and training/testing multiple times.  \n",
        "- *Example:* In k-fold cross-validation, the dataset is split into k subsets, and the model is trained on k-1 folds while the remaining fold is used for testing. This process is repeated k times.\n",
        "\n",
        "**Validation Accuracy:**  \n",
        "This metric measures how well a model performs on unseen data during the tuning process, helping you gauge its ability to generalize.  \n",
        "- *Example:* If a model achieves a validation accuracy of 85%, it means that, on average, it correctly predicts 85% of the outcomes on the validation sets during cross-validation.\n",
        "\n",
        "**Overfitting:**  \n",
        "  Hyperparameter tuning can inadvertently lead to overfitting if tuned aggressively on limited validation data.  \n",
        "  - *Example:* Achieving very high validation accuracy during hyperparameter search but significantly worse performance on new unseen data.\n",
        "\n",
        "**Early Stopping:**  \n",
        "  A technique to halt training if performance on a validation set stops improving, thus preventing overfitting and reducing unnecessary training time.  \n",
        "  - *Example:* Stopping neural network training when validation accuracy does not improve after 10 consecutive epochs.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### **6.3 Objective**\n",
        "\n",
        "In this exercise, you'll explore the critical process of **hyperparameter tuning**, which involves systematically adjusting model settings to find the optimal balance between accuracy, complexity, and training time. You'll practice methods such as **Grid Search** to improve classifier performance, gaining experience with **`GridSearchCV`** from **`scikit-learn`**.\n",
        "\n",
        "This exercise will demonstrate how choosing the right hyperparameters can significantly improve your model’s predictive power.\n",
        "\n",
        "### **6.3 Objective**\n",
        "\n",
        "In this exercise, you'll practice **hyperparameter tuning**, an essential step for maximizing your classifier's performance. You'll explore how systematically adjusting hyperparameters impacts model accuracy and generalization.\n",
        "\n",
        "> **Exercise 6**  \n",
        "> Perform hyperparameter tuning and evaluate its effect on model performance:\n",
        ">\n",
        "> 1. **Hyperparameter Tuning with Grid Search**  \n",
        ">   - Choose one classifier (such as `RandomForestClassifier` or `MLPClassifier`) to perform hyperparameter tuning.\n",
        ">   - Define a hyperparameter grid containing a few hyperparameters relevant to your chosen classifier.\n",
        ">     - *Example (MLPClassifier):* number of hidden layers, learning rates, activation functions, or regularization strengths.\n",
        ">   - Perform a **grid search** using `GridSearchCV` from `scikit-learn`:\n",
        ">     - Set the cross-validation folds (`cv`) to at least 3 for reliable estimates.\n",
        ">     - Consider using `n_jobs=-1` to leverage multiple CPU cores for efficiency.\n",
        ">     - Clearly document the hyperparameters you're exploring.\n",
        ">\n",
        "> 2. **Evaluate and Compare Tuned vs. Untuned Models**  \n",
        ">   - After your grid search finishes, clearly print the best hyperparameters found and their associated validation accuracy (`best_score_`).\n",
        ">   - Using these best hyperparameters, evaluate your tuned classifier on the separate test set. Report this test accuracy clearly.\n",
        ">   - Train a **default** (untuned) classifier with identical conditions, evaluate its performance on the same test set, and compare its accuracy with the tuned model.\n",
        ">\n",
        "> 3. **Reflect on Your Results**  \n",
        ">   - Briefly discuss your findings:\n",
        ">     - Did hyperparameter tuning improve accuracy? If so, by how much?\n",
        ">     - What might these results suggest about the dataset or your chosen classifier?\n",
        "\n",
        "**Additional Recommendations:**  \n",
        "- Clearly comment your code for each step, particularly when defining hyperparameter grids, to document your thought process.  \n",
        "- If interested, experiment with using **`RandomizedSearchCV`** (optional) to see how random sampling can speed up hyperparameter optimization for larger hyperparameter spaces.  \n",
        "- As always, use descriptive variable naming to enhance code readability, aiding both your own understanding and that of your peers.\n"
      ],
      "metadata": {
        "id": "DBy2PCfV6p9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Hyperparameter Search\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WEyzuTYO6p9b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T07:22:13.781676Z",
          "iopub.execute_input": "2025-03-02T07:22:13.781994Z",
          "iopub.status.idle": "2025-03-02T07:22:21.856018Z",
          "shell.execute_reply.started": "2025-03-02T07:22:13.781971Z",
          "shell.execute_reply": "2025-03-02T07:22:21.855178Z"
        },
        "id": "sT9al0iH6p9b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Evaluate and Compare"
      ],
      "metadata": {
        "id": "x6VsLn9M6p9b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T07:22:21.857190Z",
          "iopub.execute_input": "2025-03-02T07:22:21.857481Z",
          "iopub.status.idle": "2025-03-02T07:22:22.742330Z",
          "shell.execute_reply.started": "2025-03-02T07:22:21.857459Z",
          "shell.execute_reply": "2025-03-02T07:22:22.741449Z"
        },
        "id": "EGJQcGsD6p9b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Reflection"
      ],
      "metadata": {
        "id": "31G87u-N6p9b"
      }
    }
  ]
}